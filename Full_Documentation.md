# Full Documentation

## Web scraping 

In our project, we've tailored a web scraping approach that meticulously caters to the distinct characteristics of each news source we target, such as Strait Times and CNA. The necessity for individual scrapers stems from the unique web page structures and encoding practices employed by different news outlets. Each scraper is engineered to extract relevant data effectively, considering the unique HTML/CSS structures of each source and are each housed in its dedicated folder. This organisation not only enhances the efficiency of our data extraction but also ensures precision by acknowledging the specific nuances of each website. 

Once we gather the data, it's immediately saved as a CSV file within the corresponding folder of the scrapper. Following the initial extraction, we manually transfer these CSV files into a central 'csvs' folder in Github. This manual intervention allows us to carefully inspect and verify the data before it moves into the analysis phase. Such a procedure is integral to our workflow, providing a safeguard against the risk of data loss in the event that we encounter an error with one of the files. 

## Sentiment Analysis

With our CSV files consolidated, we proceed to sentiment analysis. Here, we employ five advanced models, Financial, FinBERT, Sigma, Soleimanian, and Yiyanghkust, to analyse the news content within the CSV files, classifying sentiments as positive, negative, or neutral. Firstly, we read our consolidated CSV files from the central ‘csvs’ folder into a single dataframe. Next, we begin preprocessing our data within the dataframe to prepare our dataframe for accurate analysis. We begin by removing any null values and duplicate entries, ensuring the cleanliness and integrity of our dataset. We then proceed to refine the text data through various transformations aimed at enhancing its analytical utility. We start off by converting all text to lower case, establishing uniformity across the dataset. We follow this by stripping away common stopwords, isolated characters, and any excess whitespace, each step methodically reducing the noise within our data. Punctuation marks, emojis, and other non-textual symbols are also removed to focus solely on meaningful text content.The final step in our preprocessing routine is lemmatization. This process standardises various forms of a word into a single base form, allowing our sentiment analysis algorithms to interpret and analyse the text more effectively. Through these comprehensive preprocessing measures, we ensure that our dataset is optimised for sentiment analysis, setting a solid foundation for the subsequent analytical processes to derive nuanced and accurate insights from the news content.

After preprocessing all the data, we visualise the outcomes of each sentiment model to help us to assess the models' performance and reliability. Based on our evaluations, we select the Financial, FinBERT, and Sigma models for further analysis due to their consistent and reliable outputs.The final phase involves applying these selected models to the data again with this time focusing on sector-specific analysis. The resulting sentiments are then visualised through pie charts and stacked bar charts, providing clear and actionable insights into sectoral sentiment trends. In our sector-specific analysis, we examine different industries to uncover distinct sentiment trends. For example, we might find that one sector shows generally positive feelings, while another faces negative views. It's important for stakeholders to recognize these varied sentiment patterns across industries. Such insights can significantly impact their decisions, strategies, and how they manage risks, offering a clearer picture of each sector's sentiment landscape and helping to navigate the business environment effectively.

## Topic Modelling 

In our topic modelling implementation, we used Latent Dirichlet Allocation (LDA) to analyse the same text data. The data preprocessing procedures were very similar to the steps mentioned above, including removing duplicates, empty rows, punctuation, numbers, single characters, hyperlinks, and extra white space. Similarly, this is followed by removing stopwords, tokenization and converting test to lowercase and finally lemmatization. Additionally in topic modelling, we added Bigrams (Phrases consisting of two words occurring together frequently) to the tokenized documents such that the model captures meaningful phrases or expressions that consist of two words occurring together frequently, thereby retaining contextual relationship and improving the quality of our topic interpretation. A dictionary is also created to map unique tokens to numerical IDs and generates a corpus. This transforms the raw text data into a structured numerical format such that our model would be able to analyse the text, serving as a pipeline for our topic model. Finally, we built our model with iteration over a range of topic numbers (From 2 to 20 topics), fitting LDA models with different numbers of topics to the corpus. For each model, it computes the coherence score, which measures the interpretability and coherence of the topics generated by the model. The coherence scores for different numbers of topics are then plotted to determine the optimal number of topics for our dataset.

## Dashboard

In the dashboard integration stage, we utilised streamlite to retrieve and visualise the sentiment analysis outputs and coherence score visualisations into their respective 'dashboard/csvs' and 'dashboard/assets' folders, presenting tailored visualisation corresponding to contents of each file. This organisation enables the dashboard to automatically pull the relevant CSV files and visualisation assets, transforming this data into interactive and informative graphical representations. Through this streamlined process, the dashboard offers a clear and engaging overview of our findings, encapsulated within an intuitive interface that enhances user understanding and interaction with the data derived from our analysis.
