# UOB-Financial Loan Market Analysis

This project performs web scraping, sentiment analysis, and topic modeling on the financial loan market based on a few sectors. It also includes an interactive dashboard to visualize the findings.

## Features

- **Web Scraping:** Uses `Selenium` to scrape data from the financial loan market for specific sectors.
- **Sentiment Analysis:** Uses models from Hugging Face's `transformers` library to analyze the sentiment of the scraped data and understand the market sentiment for each sector.
- **Topic Modeling:** Performs topic modeling to identify the main topics in the market discussion for each sector.
- **Interactive Dashboard:** Uses `Streamlit` to create an interactive dashboard that visualizes the results of the sentiment analysis and topic modeling.

## Installation

1. Clone this repository.
  ```bash
  git clone https://github.com/GreyScaling/UOB-Financial-Loan-Analysis/
  ```
   
2. Install the required packages:
  ```bash
  pip install -r requirements.txt
  ```

## Running the Dashboard

Follow these steps to view the interactive dashboard:

1. Open your terminal.

2. Navigate to the `dashboard` directory:

 ```bash
 cd dashboard
 ```
3. Run the streamlit app :
  ```bash
  streamlit run app.py
  ```

## Topic Modeling Pipeline Documentation
### Introduction
This documentation provides an overview of the topic modeling pipeline implemented for analyzing a collection of documents. The pipeline includes data preprocessing, tokenization, phrase modeling, dictionary and corpus creation, topic modeling using Latent Dirichlet Allocation (LDA), and visualization of the results.

#### Components
1. Data Preprocessing
Description: This step involves cleaning and preprocessing the raw text data to make it suitable for further analysis.
Preprocessing Steps:
Convert text to lowercase
Remove punctuation, stopwords, single characters, hyperlinks, and extra whitespace
Lemmatize words
2. Tokenization
Description: Tokenization is the process of splitting the preprocessed text into individual words or tokens.
Libraries Used: RegexpTokenizer from NLTK
3. Phrase Modeling
Description: This step identifies and constructs bigrams (two-word phrases) within the tokenized documents to capture meaningful phrases.
Libraries Used: Phrases from Gensim
4. Creating Dictionary and Corpus
Description: A dictionary is created to map each unique word to a unique integer ID. The tokenized documents are then converted into bag-of-words corpus format for input to the LDA model.
Libraries Used: Dictionary and corpus from Gensim
5. Topic Modeling with LDA
Description: Latent Dirichlet Allocation (LDA) is a probabilistic model used for topic modeling. It discovers abstract topics from a collection of documents by assigning each word in the document a probability of belonging to each topic.
Libraries Used: LdaModel from Gensim
Parameters:
Number of topics
Random seed
Iterations
Alpha and Eta (hyperparameters)
Passes
Coherence Score: Coherence score is calculated to evaluate the quality of topics generated by the model.
6. Visualization
Description: Interactive visualization is generated using pyLDAvis to explore the topics and coherence scores.
Libraries Used: pyLDAvis and pyLDAvis.gensim
Usage
Input: Provide a collection of documents in CSV format.
Output: Interactive visualization showing topics and coherence scores.
Parameters: Tune parameters such as the number of topics, iterations, and hyperparameters for LDA model optimization.
