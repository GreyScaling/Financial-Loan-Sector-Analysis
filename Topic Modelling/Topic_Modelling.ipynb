{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for data manipulation, NLP, and visualization\n",
    "import pandas as pd  # Importing pandas for data manipulation\n",
    "import numpy as np  # Importing numpy for numerical operations\n",
    "import re  # Importing re for regular expressions\n",
    "import dask  # Importing dask for parallel computing\n",
    "dask.config.set({'dataframe.query-planning': True})  # Configuring dask for query planning\n",
    "import dask.dataframe as dd  # Importing dask dataframe for big data processing\n",
    "\n",
    "# NLP Libraries\n",
    "from nltk.corpus import stopwords  # Importing NLTK's stopwords\n",
    "import spacy  # Importing spaCy for advanced NLP tasks\n",
    "from nltk.tokenize import RegexpTokenizer  # Importing RegexpTokenizer for tokenization\n",
    "from gensim.models import Phrases  # Importing Phrases for phrase modeling\n",
    "from gensim.corpora import Dictionary  # Importing Dictionary for building corpora\n",
    "from gensim.models.ldamulticore import LdaMulticore  # Importing LdaMulticore for topic modeling\n",
    "from nltk.probability import FreqDist  # Importing FreqDist for frequency distribution\n",
    "from gensim.parsing.preprocessing import remove_stopwords  # Importing remove_stopwords for preprocessing\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer  # Importing SentimentIntensityAnalyzer for sentiment analysis\n",
    "\n",
    "# Visualisation Libraries\n",
    "import matplotlib.pyplot as plt  # Importing matplotlib for basic visualization\n",
    "from plotly.subplots import make_subplots  # Importing make_subplots for subplots\n",
    "import plotly.graph_objects as go  # Importing graph_objects for plotly visualizations\n",
    "import seaborn as sns  # Importing seaborn for advanced visualization\n",
    "import plotly.offline as pyo  # Importing plotly for interactive plots\n",
    "import pyLDAvis  # Importing pyLDAvis for topic visualization\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "\n",
    "# Setting matplotlib style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Initializing plotly for offline use\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "# Setting seaborn context for paper\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"GreyScaling\"\n",
    "repo_name = \"UOB-Financial-Loan-Analysis\"\n",
    "\n",
    "# Specify the folder path (e.g., \"path/to/folder\")\n",
    "folder_path = \"csvs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(username , repo_name , folder_path):\n",
    "\n",
    "    '''\n",
    "\n",
    "    Takes in the github credentials and returns the csv filenames within the folder given\n",
    "    \n",
    "    Parameters: \n",
    "        username (str): GitHub Username\n",
    "        repo_name (str): User's Repository Name\n",
    "        folder_path (str): The folder path containing the csv files\n",
    "           \n",
    "    Returns:\n",
    "\n",
    "        filenames (list) : Returns a list of csv folderpaths    \n",
    "     '''\n",
    "\n",
    "    url = f\"https://api.github.com/repos/{username}/{repo_name}/contents/{folder_path}\"\n",
    "    csv_path = f'https://raw.githubusercontent.com/{username}/{repo_name}/main/{folder_path}/'\n",
    "\n",
    "\n",
    "    filenames = []\n",
    "    response = requests.get(url)\n",
    "\n",
    "    #Checks if the url is OK\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        for item in data:\n",
    "            # Check if the item is a file and ends with \".csv\" extension\n",
    "            if item[\"type\"] == \"file\" and item[\"name\"].endswith(\".csv\"):\n",
    "\n",
    "                #Attach the folder path together with the filename get the entire url  \n",
    "                filename = csv_path + item['name']\n",
    "                filenames.append(filename)\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "\n",
    "    return(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs = get_filenames(username , repo_name , folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all the files into a single dataframe\n",
    "df = dd.read_csv(csvs).compute().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows based on all columns\n",
    "df[df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(dataframe):\n",
    "    '''\n",
    "    This Function performs the pre-processing of the dataframe \n",
    "\n",
    "    args:\n",
    "        dataframe: The dataframe to be preprocessed\n",
    "    \n",
    "    Functions:\n",
    "        \n",
    "        the following are the steps taken to proprocess the data\n",
    "\n",
    "        1) Lower case tranformation\n",
    "        2) Removing Stopwords\n",
    "        3) Removing single alphabets\n",
    "        4) Removing WhiteSpace\n",
    "        5) Removing Punctuations\n",
    "        6) Removing Emojis & other image related symbolds\n",
    "        7) Lemmatizing the words\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #converts text to lowercase\n",
    "    dataframe['Content'] = dataframe['Content'].apply(lambda x: x.lower())\n",
    "\n",
    "    #Remove all punctuations\n",
    "    dataframe['Content'] = dataframe['Content'].apply(lambda x: re.sub('[^\\w\\s]', ' ', x))\n",
    "\n",
    "    #Remove all stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['from', 'singapore', 'malaysia' , 'said' ,','])\n",
    "    dataframe['Content'] = dataframe['Content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "    #Removing Numbers\n",
    "    dataframe['Content'] = dataframe['Content'].apply(lambda x: re.sub('\\d+' , '' , x))\n",
    "\n",
    "    #remove single alphabets\n",
    "    dataframe['Content'] = dataframe['Content'].apply(lambda x: re.sub('\\b[a-zA-Z]\\s', ' ' , x))\n",
    "\n",
    "    #remove Hyperlinks\n",
    "    dataframe['Content'] = dataframe['Content'].apply(lambda x: re.sub(\"<.*?>+|https?://\\S+|www\\.\\S+\", \" \" , x))\n",
    "\n",
    "    dataframe['Content'] = dataframe['Content'].apply(lambda x: re.sub(\"/^\\s+|\\s+$|\\s+(?=\\s)/g\", \"\" , x))\n",
    "\n",
    "\n",
    "    #Removing Emojis and any other image related symbols\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoji\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\" \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    dataframe['Content'] = dataframe['Content'].apply(lambda x: emoji_pattern.sub(r' ', x))\n",
    "\n",
    "    #Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    dataframe['Content'] = dataframe['Content'].apply(lambda x:\" \".join([lemmatizer.lemmatize(word) for word in x.split(' ')]) ) \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
